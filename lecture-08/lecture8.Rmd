---
title: 'Lecture 8: Classifiers'
author: by Jeff Chen & Dan Hammer, Georgetown University McCourt School of Public
  Policy
subtitle: Intro to Data Science for Public Policy, Spring 2016
output:
  pdf_document:
    toc: yes
  html_document:
    theme: journal
    toc: yes
---

Building on Lecture 7, this section lightly introduces three more classification algorithms: Generalized Linear Models, Support Vector Machines, and Artificial Neural Networks.

##


##Support vector machines
SVMs cannot be interpretted. They are    
You’re given a three feature data set. Two features are continuous inputs and the third is a target containing labels of two groups. Upon plotting the points in two dimensional space and color coding for the target, you notice that there is a clear line of separation: A straight line can partition one group from the other (Figure 1). You think and realize that multiple lines could do the job: there are almost infinite lines (Figure 2) that could serve as the boundary between the groups. But which is the best? There should in theory be one line that optimally describes the separation between the groups. 

```{r, echo = FALSE, message = FALSE, warning= FALSE}
#Margin Example
margin_size <- 0.3
set.seed(123)
df <- data.frame(x = runif(200),
                 y = runif(200),
                 supports = NA)

#Set up margin supports
  supports <- data.frame( x = c(0.6, 0.7, 0.7), y = NA, supports = NA)
  supports$supports[1:2] <- -1.08 + 2*supports$x[1:2]
  supports$supports[3] <- -.52 + 2*supports$x[3]
  
df <- rbind(df,
            supports)
  
  
#Best boundary
  df$z <- -0.8 + df$x*2 
  df$perp <- 0.6578033 + df$x*-0.5
  df$perp[df$x >= 0.6951213] <- NA
  df$perp[df$x <= 0.4711213] <- NA
  
#Cut out
  df <- df[which((df$y > df$z + margin_size | df$y < df$z - margin_size | !is.na(df$supports))), ]
  df$group <- "Side A"
  df$group[df$y < df$z - margin_size] <- "Side B"
  df$cols <- "blue"
  df$cols[df$group == "Side B"] <- "green"
  
  
#Alternative boundaries
  df$z1 <- -1.1 + df$x*2.1
  df$z2 <- -0.5 + df$x*1.9
  df$z3 <- -0.95 + df$x*2  
  df$z4 <- -0.65 + df$x*2  
  df$z5 <- -0.95 + df$x*2.3
  df$z6 <- -0.65 + df$x*1.7
  
  df$margin2 <- -1.08 + df$x*2
  df$margin1 <- -.52 + df$x*2
  
  df <- df[order(df$perp),]
  
#Plot
library(ggplot2)

base <- ggplot(df, aes(group=factor(group))) + 
    geom_point(aes(x = x, y = y,  colour = factor(group)))  +
    ylim(0,1) + xlim(0,1) + 
    ylab("x1") + xlab("x2") +
    ggtitle("(1)") + scale_colour_manual(values=c("lightblue", "lightgrey")) +
  coord_fixed(ratio = 1) +
    theme(plot.title = element_text(size = 10), 
          axis.line=element_blank(),
          axis.text.x=element_blank(),
          axis.text.y=element_blank(),axis.ticks=element_blank(),
          legend.position="none",
          panel.background=element_blank(),panel.border=element_blank(),
          panel.grid.major=element_blank(),
          panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm"))
  
options1 <- ggplot(df) + 
  geom_point(aes(x = x, y = y, colour = df$cols)) +
  geom_line(aes(x = x, y = z), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z1), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z2), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z3), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z4), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z5), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z6), alpha = 0.5, colour = "grey") + 
  ylim(0,1) + xlim(0,1) +
  ggtitle("(2)") +  scale_colour_manual(values=c("lightblue", "lightgrey")) +
  coord_fixed(ratio = 1) + 
  ylab("x1") + xlab("x2") +
  theme(plot.title = element_text(size = 10), 
        axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm"))


optimal <- ggplot(df) + 
  geom_point(aes(x = x, y = y, colour = df$cols)) +
  geom_line(aes(x = x, y = z), size = 2, colour = "red") + 
  geom_line(aes(x = x, y = margin1), size = 1, linetype="dashed", colour = "blue") + 
  geom_line(aes(x = x, y = margin2), size = 1, linetype="dashed", colour = "blue") + 
  geom_line(aes(x = x, y = perp), size = 1, colour = "blue") + 
  ylim(0,1) + xlim(0,1) + 
  ylab("x1") + xlab("x2") +
  ggtitle("(3)") +  scale_colour_manual(values=c("lightblue", "lightgrey")) +
  coord_fixed(ratio = 1) +
  theme(plot.title = element_text(size = 10), 
        axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm")) + 
  annotate("text", x = .75, y = .9, label = "H1", colour = "blue") + 
  annotate("text", x = .93, y = .85, label = "H2", colour = "blue") + 
  annotate("text", x = .52, y = .45, label = "d+", colour = "blue") + 
  annotate("text", x = .68, y = .37, label = "d-", colour = "blue") + 
  annotate("text", x = .2, y = .1, label = 'atop(bold("wx-b = +1"))', colour = "blue", parse = T)  + 
  annotate("text", x = .39, y = .2, label = 'atop(bold("wx-b = 0"))', colour = "red", parse = T) + 
  annotate("text", x = .75, y = .1, label = 'atop(bold("wx-b = -1"))', colour = "blue", parse = T) 

  
supports <- ggplot(df) + 
  geom_point(aes(x = x, y = y, colour = df$cols)) +
  geom_line(aes(x = x, y = z), size = 2, colour = "red") + 
  geom_line(aes(x = x, y = margin1), size = 1, linetype="dashed", colour = "blue") + 
  geom_line(aes(x = x, y = margin2), size = 1, linetype="dashed", colour = "blue") +
  geom_point(aes(x = x, y = supports, colour = "red", size=0.7)) +
  ylim(0,1) + xlim(0,1) + 
  ylab("x1") + xlab("x2") +
  ggtitle("(4)") +  scale_colour_manual(values=c("lightblue", "lightgrey", "green")) +
  coord_fixed(ratio = 1) +theme_bw() +
  theme(plot.title = element_text(size = 10), 
        axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm")) 

library(gridExtra)
grid.arrange(base, options1, ncol = 2)
```

If we are to assume a straight line is appropriate, we can find a line that maximizes the distance between the groups. As seen in Figure 3, the dashed grey lines and the solid purple lines are known as hyperplanes, but are simply lines in two dimensional space. H1 and H2 are hyperplanes that are defined by a set of "support vectors" -- points that serve as control  or reference points for the location of the hyperplane (see Figure 4).  The elegance of this method is that not all points in a dataset are used to define H1 and H2: only select points on or near the hyperplanes are required to define the plane. These planes are defined using simple linear equations shown in dot-product form:  $$w^T x - b = +1$$  and $$w^T x - b = -1$$ for H2, where $w$ is a weight that needs to be calibrated.  H1 and H2 primarily serve as the boundaries of what is known as the *margin*, or the space that maximally separates the two classes that are linearly separable The optimal hyperplane or *decision boundary* is defined as $$w^T x - b = 0$$ and sits at a distance of $d+$ from H1 and $d-$ from H2.


```{r, echo=F, warning=FALSE, message=FALSE}
grid.arrange(optimal, supports, ncol = 2)
```



[Soft margin versus hard margin]

[Higher dimensions]

[Pseudo code]

[Math]

[Cost and Gamma]

Start with line
$mx + b = y$

Generalized case for plane
$wt + b = y$

Classification for two classes
$w^T + b > y$
$w^T + b < y$
$w^T + b$ are parameters of plane

Where $y = 0$, that’s when we have no idea if point is positive or neg
labels are $y \in {-1, +1}$

boundaries are thus -1 and 1. Above the absolutes are scored as 1 and everything in between is iffy
$w^T + b = +1$
$w^T + b = 0$
$w^T + b = -1$
Lines are parallel to one another

To optimize: we want to find the maximum distance between H1 and H2. This can be done by finding the distance of the line that is perpendicular to H1 and H2 since they are parallel.  The following equations are the points at which the perpendicular line intersects at two points
$w^T_1 + b =1$
$w^T_2 + b =-1$

Subtract the two equations: 
$w^T(x_1 - x_2) = 2$
Using a vector calculus trick, we can $x_1 - x_2 = \frac{2}{||w||}$ where $||w||$ is the normalized w vector (length of w)
This is the definition of the margin = $\frac{2}{||w||}$

Optimization
- maximize = $\frac{2}{||w||}$
- easier to minimize $\frac{1}{2}||w||^2$ — this is known as a quadratic programming problem
Or maximize another equation: sum of all points i, minus product alphas, labels, values
$w(\alpha) = \sum_i{\alpha_i} - \frac{1}{2}\sum_i{\alpha_1\alpha_0 y_1 y_0 x_1^Tx_0}$
subject to $\alpha_i \geq 0$ (non-negatives), $\sum_i{\alpha_i y_i} = 0$ (sum of alpha and y are equal to zero)

Once the equation is maximized, $w = \sum_i{\alpha_i y_i x_i}$ can be extracted
Many $\alpha_i$ are zero, non-zeros are the support vectors or basically data points 
Only a few X's matter

$x_i^Tx_j$ is the dot product that indicates relationship and only cases that move together are used

####Applying SVMs
```{r, message = FALSE, warning = FALSE, eval = F}
library(e1071)
health <- read.csv("data/lecture8.csv")

#Create index of randomized booleans of the same length as the health data set
  set.seed(100)
  rand <- runif(nrow(health)) 
  rand <- rand > 0.5
  
#Create train test sets
  train <- health[rand == T, ]
  test <- health[rand == F, ]
  
#Check SVM
  svm.fit <- svm(coverage ~ ., data=train)
  summary(svm.fit)
  
#Tune SVM
  tune <- tune.svm(coverage ~. ,
                    data = train,
                    kernel="radial", 
                    cost=10^(-1:2), gamma=c(.5,1,2))

  print(tune)
  
#Re-train
  svm.fit2 <- svm(coverage ~ ., data=train, kernel="polynomial", cost=1, gamma=1)
  summary(svm.fit2)
  
#Prediction
  svm.pred <- predict(svm.fit2, test)
```

Estimate the AUC
```{r, message = FALSE, warning = FALSE, eval = F}
#plotROC
  library(plotROC)
  library(ggplot2)

#Predict values for train set
  pred.svm.train <- predict(svm.fit2, train, type='prob')

#Predict values for test set
  pred.svm.test <- predict(svm.fit2, test, type='prob')

  
#Set up ROC inputs
  input.svm <- rbind(data.frame(model = "train", d = train$coverage, m = pred.svm.train), 
                  data.frame(model = "test", d = test$coverage,  m = pred.svm.test))
  
#Graph all three ROCs
  roc.svm <- ggplot(input.svm, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  + ggtitle("ROC: SVM")

#AUC
  calc_auc(roc.svm)
```




##Artificial Neural Networks
[]

##Applications of classifiers
###Appropriate uses of classification techniques
[text goes here]

```{r}
#
```
###Scoring
[text goes here]

```{r}
#
```

###prediction and prioritization
[text goes here]

```{r}
#
```

###Propensity score matching
[text goes here]

```{r}
#
```


###Exercise Data
- [Labor and wage analysis]