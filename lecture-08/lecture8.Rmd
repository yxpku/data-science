---
title: 'Lecture 8: Classifiers'
author: by Jeff Chen & Dan Hammer, Georgetown University McCourt School of Public
  Policy
subtitle: Intro to Data Science for Public Policy, Spring 2016
output:
  pdf_document:
    toc: yes
  html_document:
    theme: journal
    toc: yes
---

Building on Lecture 7, this section lightly introduces three more classification algorithms: Generalized Linear Models, Support Vector Machines, and Artificial Neural Networks.

##


##Support vector machines
SVMs cannot be interpretted. They are    
You’re given a three feature data set. Two features are continuous inputs and the third is a target containing labels of two groups. Upon plotting the points in two dimensional space and color coding for the target, you notice that there is a clear line of separation: A straight line can partition one group from the other (Figure 1). You think and realize that multiple lines could do the job: there are almost infinite lines (Figure 2) that could serve as the boundary between the groups. But which is the best? There should in theory be one line that optimally describes the separation between the groups. 

```{r, echo = FALSE, message = FALSE, warning= FALSE}
#Margin Example
margin_size <- 0.3
set.seed(123)
df <- data.frame(x = runif(200),
                 y = runif(200),
                 supports = NA)

#Set up margin supports
  supports <- data.frame( x = c(0.6, 0.7, 0.7), y = NA, supports = NA)
  supports$supports[1:2] <- -1.08 + 2*supports$x[1:2]
  supports$supports[3] <- -.52 + 2*supports$x[3]
  
df <- rbind(df,
            supports)
  
  
#Best boundary
  df$z <- -0.8 + df$x*2 
  df$perp <- 0.6578033 + df$x*-0.5
  df$perp[df$x >= 0.6951213] <- NA
  df$perp[df$x <= 0.4711213] <- NA
  
#Cut out
  df <- df[which((df$y > df$z + margin_size | df$y < df$z - margin_size | !is.na(df$supports))), ]
  df$group <- "Side A"
  df$group[df$y < df$z - margin_size] <- "Side B"
  df$cols <- "blue"
  df$cols[df$group == "Side B"] <- "green"
  
  
#Alternative boundaries
  df$z1 <- -1.1 + df$x*2.1
  df$z2 <- -0.5 + df$x*1.9
  df$z3 <- -0.95 + df$x*2  
  df$z4 <- -0.65 + df$x*2  
  df$z5 <- -0.95 + df$x*2.3
  df$z6 <- -0.65 + df$x*1.7
  
  df$margin2 <- -1.08 + df$x*2
  df$margin1 <- -.52 + df$x*2
  
  df <- df[order(df$perp),]
  
#Plot
library(ggplot2)

base <- ggplot(df, aes(group=factor(group))) + 
    geom_point(aes(x = x, y = y,  colour = factor(group)))  +
    ylim(0,1) + xlim(0,1) + 
    ylab("x1") + xlab("x2") +
    ggtitle("(1)") + scale_colour_manual(values=c("lightblue", "lightgrey")) +
  coord_fixed(ratio = 1) +
    theme(plot.title = element_text(size = 10), 
          axis.line=element_blank(),
          axis.text.x=element_blank(),
          axis.text.y=element_blank(),axis.ticks=element_blank(),
          legend.position="none",
          panel.background=element_blank(),panel.border=element_blank(),
          panel.grid.major=element_blank(),
          panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm"))
  
options1 <- ggplot(df) + 
  geom_point(aes(x = x, y = y, colour = df$cols)) +
  geom_line(aes(x = x, y = z), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z1), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z2), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z3), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z4), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z5), alpha = 0.5, colour = "grey") + 
  geom_line(aes(x = x, y = z6), alpha = 0.5, colour = "grey") + 
  ylim(0,1) + xlim(0,1) +
  ggtitle("(2)") +  scale_colour_manual(values=c("lightblue", "lightgrey")) +
  coord_fixed(ratio = 1) + 
  ylab("x1") + xlab("x2") +
  theme(plot.title = element_text(size = 10), 
        axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm"))


optimal <- ggplot(df) + 
  geom_point(aes(x = x, y = y, colour = df$cols)) +
  geom_line(aes(x = x, y = z), size = 2, colour = "purple") + 
  geom_line(aes(x = x, y = margin1), size = 1, linetype="dashed", colour = "grey") + 
  geom_line(aes(x = x, y = margin2), size = 1, linetype="dashed", colour = "grey") + 
  geom_line(aes(x = x, y = perp), size = 1, colour = "blue") + 
  ylim(0,1) + xlim(0,1) + 
  ylab("x1") + xlab("x2") +
  ggtitle("(3)") +  scale_colour_manual(values=c("lightblue", "lightgrey")) +
  coord_fixed(ratio = 1) +
  theme(plot.title = element_text(size = 10), 
        axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm")) + 
  annotate("text", x = .3, y = .4, label = "Margin", colour = "blue") + 
  annotate("text", x = .8, y = .2, label = "Hyperplane", colour = "purple") 

  
supports <- ggplot(df) + 
  geom_point(aes(x = x, y = y, colour = df$cols)) +
  geom_line(aes(x = x, y = z), size = 2, colour = "purple") + 
  geom_line(aes(x = x, y = margin1), size = 1, linetype="dashed", colour = "grey") + 
  geom_line(aes(x = x, y = margin2), size = 1, linetype="dashed", colour = "grey") +
  geom_point(aes(x = x, y = supports, colour = "red", size=0.7)) +
  ylim(0,1) + xlim(0,1) + 
  ylab("x1") + xlab("x2") +
  ggtitle("(4)") +  scale_colour_manual(values=c("lightblue", "lightgrey", "red")) +
  coord_fixed(ratio = 1) +theme_bw() +
  theme(plot.title = element_text(size = 10), 
        axis.line=element_blank(),
        axis.text.x=element_blank(),
        axis.text.y=element_blank(),axis.ticks=element_blank(),
        legend.position="none",
        panel.background=element_blank(),panel.border=element_blank(),
        panel.grid.major=element_blank(),
        panel.grid.minor=element_blank(),plot.background=element_blank(),
        plot.margin=unit(c(-0.5,1,1,1), "cm")) 

library(gridExtra)
grid.arrange(base, options1, ncol = 2)
```

If we are to assume a straight line is appropriate, we can find a line that maximizes the distance between the groups. As seen in Figure 3, the line is known as the hyperplane or decision boundary (in purple) and the distance is known as the margin (in blue). This is the fundamental goal of Support Vector Machines (SVM): to maximize the margin around a hyperplane. By maximizing the margin around the hyperplane, we also improve the ability for the model to tolerate mistakes that cross into the margin.

The algorithm looks for points that lie on the edges of the margin to act as “support vectors” of the hyperplane. Otherwise stated, these are control points or reference points that serve as markers that indicate the edges of the margin.

```{r, echo=F, warning=FALSE, message=FALSE}
grid.arrange(optimal, supports, ncol = 2)
```

[Soft margin versus hard margin]

[Higher dimensions]

[Pseudo code]

[Math]

[Cost and Gamma]

####Applying SVMs
```{r}
library(e1071)
health <- read.csv("data/lecture8.csv")

#Create index of randomized booleans of the same length as the health data set
  set.seed(100)
  rand <- runif(nrow(health)) 
  rand <- rand > 0.5
  
#Create train test sets
  train <- health[rand == T, ]
  test <- health[rand == F, ]
  
#Check SVM
  svm.fit <- svm(coverage ~ ., data=train)
  summary(svm.fit)
  
#Tune SVM
  tune <- tune.svm(coverage ~. ,
                    data = train,
                    kernel="radial", 
                    cost=10^(-1:2), gamma=c(.5,1,2))

  print(tune)
  
#Re-train
  svm.fit2 <- svm(coverage ~ ., data=train, kernel="polynomial", cost=1, gamma=1)
  summary(svm.fit2)
  
#Prediction
  svm.pred <- predict(svm.fit2, test)
```

Estimate the AUC
```{r, message = FALSE, warning = FALSE}
#plotROC
  library(plotROC)
  library(ggplot2)

#Predict values for train set
  pred.svm.train <- predict(svm.fit2, train, type='prob')

#Predict values for test set
  pred.svm.test <- predict(svm.fit2, test, type='prob')

  
#Set up ROC inputs
  input.svm <- rbind(data.frame(model = "train", d = train$coverage, m = pred.svm.train), 
                  data.frame(model = "test", d = test$coverage,  m = pred.svm.test))
  
#Graph all three ROCs
  roc.svm <- ggplot(input.svm, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  + ggtitle("ROC: SVM")

#AUC
  calc_auc(roc.svm)
```




##Artificial Neural Networks
[]

##Applications of classifiers
###Appropriate uses of classification techniques
[text goes here]

```{r}
#
```
###Scoring
[text goes here]

```{r}
#
```

###prediction and prioritization
[text goes here]

```{r}
#
```

###Propensity score matching
[text goes here]

```{r}
#
```


###Exercise Data
- [Labor and wage analysis]